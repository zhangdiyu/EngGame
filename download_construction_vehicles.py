#!/usr/bin/env python3
"""
Construction Vehicles Image Download Script
ä» Wikimedia Commons çˆ¬å– Construction vehicles åˆ†ç±»ä¸‹çš„å›¾ç‰‡
åŠ¨æ€è·å–é¡µé¢å›¾ç‰‡ï¼Œç­›é€‰å·¥ç¨‹è½¦ç›¸å…³å›¾ç‰‡å¹¶ä¸‹è½½
"""

import os
import re
import urllib.request
import urllib.parse
import ssl
import time
import sys
import json
from html.parser import HTMLParser

# Fix Windows encoding
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')

# Disable SSL verification
ssl._create_default_https_context = ssl._create_unverified_context

# Save directory
SAVE_DIR = os.path.join(os.path.dirname(__file__), 'images', 'vehicles')

# Ensure directory exists
os.makedirs(SAVE_DIR, exist_ok=True)

# Wikimedia Commons API endpoint
WIKI_API = 'https://commons.wikimedia.org/w/api.php'

# å·¥ç¨‹è½¦å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºç­›é€‰ï¼‰
CONSTRUCTION_KEYWORDS = [
    # æŒ–æ˜æœºç±»
    'excavator', 'digger', 'backhoe', 'mini excavator',
    # æ¨åœŸæœºç±»
    'bulldozer', 'dozer',
    # è£…è½½æœºç±»
    'loader', 'front loader', 'wheel loader', 'skid steer', 'bobcat', 'telehandler',
    # èµ·é‡æœºç±»
    'crane', 'tower crane', 'mobile crane', 'crawler crane', 'truck crane',
    # å‹è·¯æœºç±»
    'roller', 'road roller', 'compactor', 'steamroller', 'vibratory roller',
    # æ…æ‹Œè½¦ç±»
    'mixer', 'cement mixer', 'concrete mixer', 'cement truck', 'agitator',
    # è‡ªå¸è½¦ç±»
    'dump truck', 'dumper', 'tipper', 'hauler', 'articulated hauler',
    # å¹³åœ°æœºç±»
    'grader', 'motor grader',
    # é“ºè·¯æœºç±»
    'paver', 'asphalt paver', 'road paver',
    # æ‰“æ¡©æœºç±»
    'pile driver', 'piling', 'pile hammer',
    # å‰è½¦ç±»
    'forklift', 'fork lift', 'reach stacker',
    # æ³µè½¦ç±»
    'concrete pump', 'pump truck', 'boom pump',
    # é’»æœºç±»
    'drilling rig', 'drill', 'boring machine',
    # å…¶ä»–å·¥ç¨‹è½¦
    'construction vehicle', 'construction equipment',
    'earthmover', 'earth mover', 'earth moving',
    'tractor', 'scraper', 'trencher',
    'asphalt', 'paving', 'road construction',
]

# ä¸­æ–‡åç§°æ˜ å°„
CHINESE_NAMES = {
    'excavator': 'æŒ–æ˜æœº',
    'digger': 'æŒ–æ˜æœº',
    'backhoe': 'åé“²æŒ–æ˜æœº',
    'mini excavator': 'å°å‹æŒ–æ˜æœº',
    'bulldozer': 'æ¨åœŸæœº',
    'dozer': 'æ¨åœŸæœº',
    'loader': 'è£…è½½æœº',
    'front loader': 'å‰è£…è½½æœº',
    'wheel loader': 'è½®å¼è£…è½½æœº',
    'skid steer': 'æ»‘ç§»è£…è½½æœº',
    'bobcat': 'å±±çŒ«è£…è½½æœº',
    'telehandler': 'ä¼¸ç¼©è‡‚å‰è£…è½¦',
    'crane': 'èµ·é‡æœº',
    'tower crane': 'å¡”å¼èµ·é‡æœº',
    'mobile crane': 'ç§»åŠ¨èµ·é‡æœº',
    'crawler crane': 'å±¥å¸¦èµ·é‡æœº',
    'truck crane': 'æ±½è½¦èµ·é‡æœº',
    'roller': 'å‹è·¯æœº',
    'road roller': 'å‹è·¯æœº',
    'compactor': 'å‹å®æœº',
    'steamroller': 'è’¸æ±½å‹è·¯æœº',
    'vibratory roller': 'æŒ¯åŠ¨å‹è·¯æœº',
    'mixer': 'æ…æ‹Œè½¦',
    'cement mixer': 'æ°´æ³¥æ…æ‹Œè½¦',
    'concrete mixer': 'æ··å‡åœŸæ…æ‹Œè½¦',
    'cement truck': 'æ°´æ³¥è½¦',
    'agitator': 'æ…æ‹Œè½¦',
    'dump truck': 'è‡ªå¸è½¦',
    'dumper': 'è‡ªå¸è½¦',
    'tipper': 'ç¿»æ–—è½¦',
    'hauler': 'è¿è¾“è½¦',
    'articulated hauler': 'é“°æ¥å¼è¿è¾“è½¦',
    'grader': 'å¹³åœ°æœº',
    'motor grader': 'å¹³åœ°æœº',
    'paver': 'é“ºè·¯æœº',
    'asphalt paver': 'æ²¥é’é“ºè·¯æœº',
    'road paver': 'é“ºè·¯æœº',
    'pile driver': 'æ‰“æ¡©æœº',
    'piling': 'æ‰“æ¡©æœº',
    'pile hammer': 'æ‰“æ¡©é”¤',
    'forklift': 'å‰è½¦',
    'fork lift': 'å‰è½¦',
    'reach stacker': 'æ­£é¢åŠ',
    'concrete pump': 'æ··å‡åœŸæ³µè½¦',
    'pump truck': 'æ³µè½¦',
    'boom pump': 'è‡‚æ¶æ³µè½¦',
    'drilling rig': 'é’»æœº',
    'drill': 'é’»æœº',
    'boring machine': 'æ˜è¿›æœº',
    'earthmover': 'åœŸæ–¹æœºæ¢°',
    'earth mover': 'åœŸæ–¹æœºæ¢°',
    'earth moving': 'åœŸæ–¹æœºæ¢°',
    'tractor': 'æ‹–æ‹‰æœº',
    'scraper': 'é“²è¿æœº',
    'trencher': 'æŒ–æ²Ÿæœº',
    'asphalt': 'æ²¥é’è®¾å¤‡',
    'paving': 'é“ºè·¯è®¾å¤‡',
    'road construction': 'é“è·¯æ–½å·¥è½¦',
}

def get_category_images(category_name, limit=100):
    """
    ä½¿ç”¨ Wikimedia Commons API è·å–åˆ†ç±»ä¸‹çš„å›¾ç‰‡åˆ—è¡¨
    """
    images = []
    continue_param = None

    while len(images) < limit:
        params = {
            'action': 'query',
            'list': 'categorymembers',
            'cmtitle': f'Category:{category_name}',
            'cmtype': 'file',
            'cmlimit': min(50, limit - len(images)),
            'format': 'json',
        }

        if continue_param:
            params['cmcontinue'] = continue_param

        url = f"{WIKI_API}?{urllib.parse.urlencode(params)}"

        try:
            headers = {
                'User-Agent': 'KidsEnglishFun/1.0 (Educational Project; Python)'
            }
            request = urllib.request.Request(url, headers=headers)

            with urllib.request.urlopen(request, timeout=30) as response:
                data = json.loads(response.read().decode('utf-8'))

            if 'query' in data and 'categorymembers' in data['query']:
                for member in data['query']['categorymembers']:
                    images.append(member['title'])

            # Check for continuation
            if 'continue' in data and 'cmcontinue' in data['continue']:
                continue_param = data['continue']['cmcontinue']
            else:
                break

        except Exception as e:
            print(f'  [ERROR] Failed to fetch category: {e}')
            break

    return images

def get_image_url(filename):
    """
    è·å–å›¾ç‰‡çš„å®é™…ä¸‹è½½URL
    """
    params = {
        'action': 'query',
        'titles': filename,
        'prop': 'imageinfo',
        'iiprop': 'url',
        'iiurlwidth': 256,  # è·å–ç¼©ç•¥å›¾URLï¼Œé€‚åˆå­¦ä¹ åº”ç”¨
        'format': 'json',
    }

    url = f"{WIKI_API}?{urllib.parse.urlencode(params)}"

    try:
        headers = {
            'User-Agent': 'KidsEnglishFun/1.0 (Educational Project; Python)'
        }
        request = urllib.request.Request(url, headers=headers)

        with urllib.request.urlopen(request, timeout=30) as response:
            data = json.loads(response.read().decode('utf-8'))

        pages = data.get('query', {}).get('pages', {})
        for page_id, page_info in pages.items():
            if 'imageinfo' in page_info:
                info = page_info['imageinfo'][0]
                # ä¼˜å…ˆä½¿ç”¨ç¼©ç•¥å›¾URL
                return info.get('thumburl') or info.get('url')

    except Exception as e:
        print(f'  [ERROR] Failed to get image URL: {e}')

    return None

def extract_vehicle_name(wiki_title):
    """
    ä»Wikiæ ‡é¢˜æå–è½¦è¾†åç§°
    ä¾‹å¦‚: "File:Yellow excavator in action.jpg" -> "excavator"
    """
    # ç§»é™¤ "File:" å‰ç¼€å’Œæ–‡ä»¶æ‰©å±•å
    name = wiki_title.replace('File:', '')
    name = re.sub(r'\.(jpg|jpeg|png|gif|svg|webp)$', '', name, flags=re.IGNORECASE)

    # è½¬å°å†™ç”¨äºåŒ¹é…
    name_lower = name.lower()

    # æ£€æŸ¥æ˜¯å¦åŒ¹é…å·¥ç¨‹è½¦å…³é”®è¯
    matched_keyword = None
    for keyword in CONSTRUCTION_KEYWORDS:
        if keyword in name_lower:
            matched_keyword = keyword
            break

    return name, matched_keyword

def create_safe_filename(name, keyword):
    """
    åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
    """
    # ä½¿ç”¨åŒ¹é…çš„å…³é”®è¯ä½œä¸ºåŸºç¡€åç§°
    base_name = keyword.replace(' ', '-')
    return f"construction-{base_name}.png"

def download_image(url, filename):
    """
    ä¸‹è½½å›¾ç‰‡
    """
    filepath = os.path.join(SAVE_DIR, filename)

    # Skip if file exists
    if os.path.exists(filepath):
        print(f'  [SKIP] {filename} exists')
        return True, filepath

    try:
        headers = {
            'User-Agent': 'KidsEnglishFun/1.0 (Educational Project; Python)',
            'Accept': 'image/*,*/*;q=0.8',
        }
        request = urllib.request.Request(url, headers=headers)

        with urllib.request.urlopen(request, timeout=30) as response:
            data = response.read()

        # Verify it's actually an image
        if len(data) < 100:
            raise Exception('File too small')

        with open(filepath, 'wb') as f:
            f.write(data)

        print(f'  [OK] {filename}')
        return True, filepath

    except Exception as e:
        print(f'  [FAIL] {filename}: {str(e)[:40]}')
        return False, None

def generate_js_data(downloaded_vehicles):
    """
    ç”Ÿæˆ JavaScript æ•°æ®æ ¼å¼
    """
    js_entries = []
    for vehicle in downloaded_vehicles:
        name = vehicle['name']
        chinese = vehicle['chinese']
        image = vehicle['filename']

        # æ ¼å¼åŒ–åç§°: construction-excavator -> Excavator
        display_name = name.replace('construction-', '').replace('-', ' ').title()

        js_entries.append(
            f"    {{ name: '{display_name}', chinese: '{chinese}', image: '{image}', emoji: 'ğŸš§' }}"
        )

    return js_entries

def get_subcategories(category_name):
    """
    è·å–å­åˆ†ç±»åˆ—è¡¨
    """
    subcategories = []
    params = {
        'action': 'query',
        'list': 'categorymembers',
        'cmtitle': f'Category:{category_name}',
        'cmtype': 'subcat',
        'cmlimit': 50,
        'format': 'json',
    }

    url = f"{WIKI_API}?{urllib.parse.urlencode(params)}"

    try:
        headers = {
            'User-Agent': 'KidsEnglishFun/1.0 (Educational Project; Python)'
        }
        request = urllib.request.Request(url, headers=headers)

        with urllib.request.urlopen(request, timeout=30) as response:
            data = json.loads(response.read().decode('utf-8'))

        if 'query' in data and 'categorymembers' in data['query']:
            for member in data['query']['categorymembers']:
                # æå–åˆ†ç±»åç§°ï¼ˆå»æ‰ "Category:" å‰ç¼€ï¼‰
                cat_name = member['title'].replace('Category:', '')
                subcategories.append(cat_name)

    except Exception as e:
        print(f'  [ERROR] Failed to fetch subcategories: {e}')

    return subcategories

def main():
    print('=' * 60)
    print('Construction Vehicles Image Download Script')
    print('ä» Wikimedia Commons çˆ¬å–å·¥ç¨‹è½¦å›¾ç‰‡')
    print('=' * 60)
    print(f'\nSave directory: {SAVE_DIR}')

    # Step 1: è·å–åˆ†ç±»å’Œå­åˆ†ç±»ä¸‹çš„æ‰€æœ‰å›¾ç‰‡
    print('\n[Step 1] Fetching images from Wikimedia Commons...')

    # ä¸»åˆ†ç±»
    category_images = get_category_images('Construction_vehicles', limit=200)
    print(f'  Found {len(category_images)} images in main category')

    # è·å–å­åˆ†ç±»
    subcategories = get_subcategories('Construction_vehicles')
    print(f'  Found {len(subcategories)} subcategories')

    # ä»ç›¸å…³å­åˆ†ç±»è·å–æ›´å¤šå›¾ç‰‡
    relevant_subcats = [
        'Excavators', 'Bulldozers', 'Wheel_loaders', 'Cranes_(machine)',
        'Roller_compactors', 'Concrete_mixer_trucks', 'Dump_trucks',
        'Backhoe_loaders', 'Forklifts', 'Motor_graders',
        # æ›´å¤šå­åˆ†ç±»
        'Articulated_haulers', 'Asphalt_pavers', 'Concrete_pumps',
        'Crawler_cranes', 'Mobile_cranes', 'Tower_cranes',
        'Mini_excavators', 'Skid-steer_loaders', 'Telehandlers',
        'Pile_drivers', 'Road_rollers', 'Steamrollers',
        'Scrapers_(machines)', 'Trenchers', 'Drilling_rigs'
    ]

    for subcat in relevant_subcats:
        sub_images = get_category_images(subcat, limit=50)
        category_images.extend(sub_images)
        print(f'  + {len(sub_images)} images from {subcat}')

    # å»é‡
    category_images = list(set(category_images))
    print(f'  Total unique images: {len(category_images)}')

    # Step 2: ç­›é€‰åŒ¹é…å·¥ç¨‹è½¦å…³é”®è¯çš„å›¾ç‰‡
    print('\n[Step 2] Filtering construction vehicle images...')
    matched_images = []
    seen_keywords = set()  # é¿å…é‡å¤ä¸‹è½½åŒç±»å‹è½¦è¾†

    for wiki_title in category_images:
        name, keyword = extract_vehicle_name(wiki_title)
        if keyword and keyword not in seen_keywords:
            matched_images.append({
                'wiki_title': wiki_title,
                'name': name,
                'keyword': keyword,
                'chinese': CHINESE_NAMES.get(keyword, 'å·¥ç¨‹è½¦'),
            })
            seen_keywords.add(keyword)
            print(f'  [MATCH] {keyword}: {name[:50]}...')

    print(f'\n  Matched {len(matched_images)} unique vehicle types')

    # Step 3: ä¸‹è½½å›¾ç‰‡
    print('\n[Step 3] Downloading images...')
    downloaded_vehicles = []

    for item in matched_images:
        # è·å–å›¾ç‰‡URL
        image_url = get_image_url(item['wiki_title'])
        if not image_url:
            print(f'  [SKIP] Could not get URL for: {item["keyword"]}')
            continue

        # ç”Ÿæˆæ–‡ä»¶å
        filename = create_safe_filename(item['name'], item['keyword'])

        # ä¸‹è½½
        success, filepath = download_image(image_url, filename)
        if success:
            downloaded_vehicles.append({
                'name': item['keyword'],
                'chinese': item['chinese'],
                'filename': filename,
            })

        time.sleep(3)  # å¢åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶

    # Step 4: ç”Ÿæˆ JavaScript æ•°æ®
    print('\n[Step 4] Generating JavaScript data...')
    js_entries = generate_js_data(downloaded_vehicles)

    print('\n' + '=' * 60)
    print('Generated JavaScript data for vehicles-data.js:')
    print('=' * 60)
    print('\n  // ==================== å·¥ç¨‹è½¦ç±» ====================')
    print('  constructionVehicles: [')
    for entry in js_entries:
        print(entry + ',')
    print('  ],')

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = os.path.join(os.path.dirname(__file__), 'construction_vehicles_data.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(downloaded_vehicles, f, ensure_ascii=False, indent=2)
    print(f'\n[INFO] Data saved to: {output_file}')

    print('\n' + '=' * 60)
    print(f'Done! Downloaded: {len(downloaded_vehicles)} vehicle images')
    print('=' * 60)

    return downloaded_vehicles

if __name__ == '__main__':
    main()
